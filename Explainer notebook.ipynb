{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook produced in the course 02806 - Social data analysis and visualization at DTU in Denmark, Spring 2017 [2]. The intention of this notebook is to present details about the data, reasonings for choices made and several discussions. It should give a better in depth understanding of what we have created for our final project. The requirements for the project can be found here [3].\n",
    "\n",
    "The story the group wants to present is a story about Arvind winning the lottery. \n",
    "He receives a huge amount of money and has in mind that he will move to New York City. \n",
    "He has questions such as how he can spend his money wisely by investing in a place to live.\n",
    "The Data Scientist David plays a central role, as he will be the one in charge of analyzing the different data sets in the story. Arvind has another request for David; He wants to live in a neighborhood considered good. David tells Arvind that he will figure things out and then come back to him. He is ready to do some data analysis and visualisation!\n",
    "\n",
    "** Participants: **\n",
    "\n",
    "* David Coleman\n",
    "* Arvind Iyer\n",
    "* Uyen Dan Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sets\n",
    "\n",
    "\n",
    "The group has used a set of data sets to realize the final project.\n",
    "Firstly we used a data set with NYC GeoJSON for mapping the actual NYC data (boroughs and neighborhoods) [10]. Secondly we used a data set called *Median House Value Per Sq Ft* from Zillow [11], the leading real estate and home-related information marketplace in the US. \n",
    "To look at the *quality* in different areas, the group used the 311 Service Requests data sets from 2010 to 2015. The NYC311 is a 24/7 help with more than 3,600 non-emergency government services such as noise complaints and illegal parkings [12]. The last data set we used contained individual income tax statistics with agi from 2014 in all states [19]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning\n",
    "\n",
    "Our initial idea was to understand the Real Estate market in the city and predicting the *quality of life* in a certain area, as the prices in New York City were known to be high. The intial plan to predict the future housing prices with several Zillow data sets. The problem with this idea was that our main data sets from Zillow had a lot of holes in it, which made them difficult to use. The future housing market is also dependent on a lot of outside parameters, which made it hard for us to do this prediction with a high percentage of certainty. With this is mind, we took some inspiration from the other groups to intercoperate existing 311 data sets. We still wanted to predict the *quality of an area* in terms of non-criminal complaints in a neighborhood or a borough. This way, we could somehow predict the kinds of non-criminal complaints that were most likely to happend in an area or a neighborhood, similar to what was done with crime in San Francisco. \n",
    "By looking at the frequency of certain types of complaints in an area, it was also possible for us to differ a specific area from another - and still keep the story we presented earlier. To do so, we used the parameters from \"Location\", \"Longitude\", \"Latitude\" and \"Complaint type\" from the 311 data sets. \n",
    "Also by comparing the zip codes for 311 requests and individual income tax, we could find out what kind of 311 requests were more frequent for people in an area with a certain income. To do so, we took the parameters \"Zip\" and \"Complaint type\" from the 311 and \"Zip\", \"Agi(1-6)\" and an additional row for everybody with an income in the agi bracket. \n",
    "Lastly the only Zillow data set we ended up using was the *Median House Value Per Sq Ft*, which contained data that helped us to estimate what the mean price per square foot in each borough was. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end user's experience [needs editing]\n",
    "\n",
    "Our story is about Arvind and David's quest to find where Arvind can live in terms of quality of life. The group thus made the visualizations to fit the story. We offer the user some kind of interaction with the website such as zooming in and out of a NYC map. By moving the cursor over an area, one can can see what neighborhood and what borough the cursor is currently on. \n",
    "\n",
    "What we had in mind prior creating the website was that it should be easy and intuitive for the user. The visualizations should support the story and the user should understand why the different things are on the page. \n",
    "** Explain how we want to user to experience the k-means, svm and knn, bullshitting now ** \n",
    "\n",
    "The optimal solution which we intially had in mind was for the user to choose to toggle on or off some parameters and the visualizations would update automatically. This would help an individual to find a good area after own requirements if the individual was planning on moving to NYC. By toggling on a parameter, the user would see on a map what kind of areas that fit that individual the most. As time was a big factor for each of the members in the group, we decided to only follow the story we presented earlier to find Arvind a good area to live with a limited level of interactive visualizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choices in data cleaning and preprocessing\n",
    "\n",
    "First we had to look through the relevant data sets to see the size, rows and the different parameters. The data sets we used were large and had parameters or data we were not too interested in to tell our story. A concrete example was the income data set. It contained data about all the states in the United States, while we were only interested in New York City. The data set needed to be cleaned two times to fit our purpose. Another concrete example was the Zillow data set, which also contained data about other states which we were not interested in. We cleaned it to only show data from New York City to fit our purpose. The third example was the cleaning we did with the 311 data sets. The data sets contain data from all the departments in NYC, but the only department that served our purpose was the one about housing. The data sets were thus cleaned to only contain information with where the \"Agency Name\" was \"Department of Housing Preservation and Development\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of the data set stats \n",
    "\n",
    "The first data set with NYC GeoJSON consists of 1.5 MB of data and 626 rows. We did not feel an urge to clean this, as the information was only used in the map of New York City.\n",
    "\n",
    "Before we cleaned the second data set from Zillow, it was a data set containing 5820 rows with data from 1996 to 2004. The file was 5.1 MB before cleaning, as it contained data from other cities than New York City as well. \n",
    "\n",
    "If we used the 311 data set that contained all the data from 2010-2015, we would have to deal with 7 columns and over 15 million rows [13]. The data set was big and hard to process, so we downloaded several data sets for each year instead. The data sets from 2011 to 2015 can be found in the references: [14], [15], [16], [17] and [18]. Each 311 data set is approximately 1.3 GB. \n",
    "\n",
    "The last data set with information about individual income tax statistics was cleaned to only contain data from New York State and once again cleaned to only contain the New York City data. After cleaning, the 6 MB data set contained 9265 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning tools\n",
    "\n",
    "##### Radial Basis Function Kernel\n",
    "The RBF kernel is in the form of a radial basis function [4]. We let X represent the input space and consider a function Φ: X→F such that it takes the input space X and maps it to a point in a new space F. Let us say that we have mapped all the data points from X to F. If you try to solve the normal linear svm in the new space F, we will notice that all the earlier working looks the same, except for that all the points xi are represented as Φ(xi). Instead of using the dot product, which is the natural inner product for Euclidean space, we replace it with ⟨Φ(x),Φ(y)⟩ - a representation of the natural inner product in the new space F. Let us then say that there exists a function k: X x X -> R such that k(xi,xj) = ⟨Φ(x),Φ(y)⟩. We can then replace all the dot products above with k(xi,xj) - also known as a kernel function [5].\n",
    "\n",
    "This algorithm is commonly used to support vector machine classification, which consists of supervising learning models to analyze data used for classification and regression analysis. The RBF kernel is well-studied and many SVM packages include it as a default method [7]. Kernel methods are commonly used for pattern analysis to find and study general types of relations as well [6]. \n",
    "\n",
    "We used this algorithm to implement a Support Vector based regressor as a prediction model that attempts to estimate future patterns of median house prices based on past fluctuations. Using the median value per square foot of area gave us a good understanding of the average cost of living in terms of real estate space. The general trends of the housing market as a whole are significantly easier to predict than the valuation estimation of a specific plot of land or locality.\n",
    "\n",
    "##### K-Nearest Neighbors\n",
    "The KNN algorithm is used for classification and regression, and will make predictions using the training dataset directly. *Data Sciene From Scratch* explains this algorithm this way: Imagine that you are voting soon, but you have no idea about who you are gonna vote for. By looking at the neighbors, you might get a better idea. You take relevant factors in the account to find the most accurate prediction. This is the idea behind nearest neighbors classification [8]. The KNN needs some notion of distance and an assumption that points that are close to each other are similar. This is one of the simplest algorithms for prediction, and it has some downsides such as focusing too much on the points around and assuming that they are similar. On the other hand, it can give us the general idea or prediction with the plotted parameters or factors. \n",
    "\n",
    "In the project, we used the KNN for predicting the areas or districts where some kind of complaint was most common. This gave us a general idea of where some complaints were reported more frequently than other places. This was also relevant for the presented story, as Arvind would have the chance to pick between districts or neighborhoods after what kind of preferences he has for most common complaints in the area.\n",
    "\n",
    "##### K-Means Clustering\n",
    "K-means clustering is a method used for cluster analysis in data mining. The method aims to partition ***n*** observations into ***k*** clusters, where each observation belongs to the cluster with the nearest mean based on the features that are provided. The results of the algorithm are centroids of K clusters, which can be used to label new data and labels for the training data as each data point is assigned to a single cluster. Each centroid is defined by a collection of features, and instead of defining groups before looking at the data, clustering will allow us to find and analyze groups that have formed organically [9]. The algorithm uses the squared Euclidean distance to assign each point to its nearest centroid and it recomputes the centroids by taking the mean of the assigned data points to a  centroid's cluster. To find the right number of K, we had to run the K-means clustering algorithm for a range of K values and compare the results.\n",
    "\n",
    "In this project, the K-means clustering was done from the 311 data per zip code and for the income per zip code. The reasoning for this was to comment on the relationship between the type of 311 calls and the income of the people living in that zip code, and to see if the clustered groups were similiar to each other. By combining the 311 and income data sets, we had 3.5 million rows to measure the K-means clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection [needs editing]\n",
    "\n",
    "**Talk about your model selection. How did you split the data in to test/training. Did you use cross validation?**\n",
    "\n",
    "For the housing data by Zillow, 9 of 12 months were used as training data for the SVM. The training data was then tested for three months and then predicted for the three months after that. The results were then cross validated with a linear and polynomial regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance [needs editing]\n",
    "\n",
    "Explain the model performance. How did you measure it? Are your results what you expected?\n",
    "\n",
    "To measure the results of the SVM and regressions for the housing data, we looked at the error values between the three models. ** add more here ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations [needs editing]\n",
    "\n",
    "[1] for the book\n",
    "[19] for d3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Explain the visualizations you've chosen.\n",
    "Why are they right for the story you want to tell?\n",
    "\n",
    "* Bullet point 1\n",
    "    * Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion [needs editing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good\n",
    "\n",
    "What went well was that the group managed to create something from the different data sets we found. We managed to clean the data sets and combine them in a way that it matched the story we wanted to tell. Additional to this, we managed to use three machine learning tools and visualize the results. The exercises and tools from the class helped us in realizing the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad\n",
    "\n",
    "The project was very open and we just needed to pick something relevant. There were too many choices to pick between, so we ended up wasting a lot of time on what we should go for as a theme. We ended up being unsecure with our topic and therefore changing it several times. When we finally went for something, we found out that the data sets were not sufficient enough for what we wanted to project. The reasoning for this could be that we had several topics in mind, but not the relevant data sets. We were also too indecisive and scared of choosing something simple when the project requirements were so open. We learned that sometimes you should go for something and go 100% for it rather than changing the topic several times when there is a deadline. \n",
    "\n",
    "What could be improved in our project was for example the user interaction part and the simple design. With more possible ways for the user to interact with the page and a more interesting design, the website would be more interesting. Another improvement could be making a better video after seeing all the other videos during the presentation. \n",
    "\n",
    "**add more examples if needed**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [1] S. Murray, Interactive Data Visualization for the Web, 1st ed. Sebastopol: O'Reilly Media, 2013. http://chimera.labs.oreilly.com/books/1230000000345/index.html\n",
    "\n",
    "\n",
    "* [2] \"suneman/socialdataanalysis2017\", GitHub, 2017. [Online]. Available: https://github.com/suneman/socialdataanalysis2017. [Accessed: 07- May- 2017].\n",
    "\n",
    "\n",
    "* [3] \"suneman/socialdataanalysis2017/wiki/Final-Project\", GitHub, 2017. [Online]. Available: https://github.com/suneman/socialdataanalysis2017/wiki/Final-Project. [Accessed: 07- May- 2017].\n",
    "\n",
    "\n",
    "* [4] \"The Radial Basis Function Kernel\", pages.cs.wisc.edu/~matthewb. [Online]. Available: http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf.\n",
    "\n",
    "\n",
    "* [5] \"Non-linear SVM classification with RBF kernel\", Stats.stackexchange.com. [Online]. Available: https://stats.stackexchange.com/questions/43779/non-linear-svm-classification-with-rbf-kernel. [Accessed: 07- May- 2017].\n",
    "\n",
    "\n",
    "* [6] \"Kernel Methods for Pattern Analysis\", http://read.pudn.com/. [Online]. Available: http://read.pudn.com/downloads167/ebook/769401/Kernel%20Methods%20for%20Pattern%20Analysis.pdf. \n",
    "\n",
    "\n",
    "* [7] W. SVM?, \"Why is RBF kernel used in SVM?\", Stats.stackexchange.com. [Online]. Available: https://stats.stackexchange.com/questions/172554/why-is-rbf-kernel-used-in-svm. [Accessed: 07- May- 2017].\n",
    "\n",
    "\n",
    "* [8] J. Grus, Data Science from Scratch, 1st ed. Sebastopol, Calif.: O'Reilly, 2015.\n",
    "\n",
    "\n",
    "* [9] A. Trevino, \"Introduction to K-means Clustering\", Datascience.com. [Online]. Available: https://www.datascience.com/blog/introduction-to-k-means-clustering-algorithm-learn-data-science-tutorials.\n",
    "\n",
    "\n",
    "* [10] \"Pediacities NYC Neighborhoods - CivicDashboards Catalog\", Catalog.opendata.city. [Online]. Available: http://catalog.opendata.city/dataset/pediacities-nyc-neighborhoods. \n",
    "\n",
    "\n",
    "* [11] \"Zillow Median Home Value\". [Online]. Available: http://files.zillowstatic.com/research/public/Neighborhood/Neighborhood_MedianValuePerSqft_AllHomes.csv. \n",
    "\n",
    "\n",
    "* [12] \"Our Story\", Www1.nyc.gov. [Online]. Available: http://www1.nyc.gov/311/our-story.page. [Accessed: 07- May- 2017].\n",
    "\n",
    "\n",
    "* [13] \"311 Service Requests from 2010 to Present | NYC Open Data\", Data.cityofnewyork.us. [Online]. Available: https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9. [Accessed: 07- May- 2017].\n",
    "\n",
    "\n",
    "* [14] \"311 Service Requests from 2011 NYC Open Data\", Data.cityofnewyork.us. [Online]. Available: https://data.cityofnewyork.us/dataset/311-Service-Requests-From-2011/fpz8-jqf4\n",
    "\n",
    "\n",
    "* [15] \"311 Service Requests from 2012 NYC Open Data\", Data.cityofnewyork.us. [Online]. Available: https://data.cityofnewyork.us/dataset/311-Service-Requests-From-2012/as38-8eb5\n",
    "\n",
    "\n",
    "* [16] \"311 Service Requests from 2013 NYC Open Data\", Data.cityofnewyork.us. [Online]. Available: https://data.cityofnewyork.us/dataset/311-Service-Requests-From-2013/hybb-af8n\n",
    "\n",
    "\n",
    "* [17] \"311 Service Requests from 2014 NYC Open Data\", Data.cityofnewyork.us. [Online]. Available: https://data.cityofnewyork.us/dataset/311-Service-Requests-From-2014/vtzg-7562\n",
    "\n",
    "\n",
    "* [18] \"311 Service Requests from 2015 NYC Open Data\", Data.cityofnewyork.us. [Online]. Available: https://data.cityofnewyork.us/dataset/311-Service-Requests-From-2015/57g5-etyj \n",
    "\n",
    "\n",
    "* [19] \"SOI Tax Stats - Individual Income Tax Statistics - 2014 ZIP Code Data (SOI)\", Irs.gov, 2017. [Online]. Available: https://www.irs.gov/uac/soi-tax-stats-individual-income-tax-statistics-2014-zip-code-data-soi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
